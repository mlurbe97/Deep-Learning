{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Counters:\n\n[[1.31943900e+00 1.29050400e+00 1.30348400e+00 ... 6.58438130e+07\n  1.70366900e+06 2.04187930e+07]\n [1.29835700e+00 1.27056200e+00 1.29895900e+00 ... 6.43080120e+07\n  4.33270700e+06 1.93726630e+07]\n [1.28217200e+00 1.29810500e+00 1.28717600e+00 ... 7.15462400e+07\n  5.29492800e+06 1.93492280e+07]\n ...\n [1.00086100e+00 1.00314100e+00 1.00086100e+00 ... 4.48407155e+08\n  1.98159000e+06 8.73024000e+05]\n [1.00087000e+00 1.00318500e+00 1.00086900e+00 ... 4.48996860e+08\n  1.72234800e+06 8.71582000e+05]\n [1.00088000e+00 1.00676100e+00 1.00087800e+00 ... 4.48782413e+08\n  1.67138200e+06 8.62587000e+05]]\n\nDSCR values:\n\n[ 0.  0.  0. ... 71. 71. 71.]\n\nIPC label:\n\n[1.298357 1.282172 1.28414  ... 1.00087  1.00088  1.000897]\n"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataframe = pd.read_csv(\"dataset.csv\", header=0)\n",
    "dataset = dataframe.values\n",
    "\n",
    "X1 = dataset[:,1] # Get the DSCR as RAW value.\n",
    "Y = dataset[:,0].astype(float) # Get IPC label for training as Float.\n",
    "X2 = dataset[0:,2:].astype(float) # Get 4 last IPC and 16 counter values as Float.\n",
    "\n",
    "print(\"Counters:\\n\")\n",
    "print(X2)\n",
    "print(\"\\nDSCR values:\\n\")\n",
    "print(X1)\n",
    "print(\"\\nIPC label:\\n\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DataFrame X1:\n         0    1    2    3\n0      1.0  0.0  0.0  0.0\n1      1.0  0.0  0.0  0.0\n2      1.0  0.0  0.0  0.0\n3      1.0  0.0  0.0  0.0\n4      1.0  0.0  0.0  0.0\n...    ...  ...  ...  ...\n23035  0.0  0.0  1.0  0.0\n23036  0.0  0.0  1.0  0.0\n23037  0.0  0.0  1.0  0.0\n23038  0.0  0.0  1.0  0.0\n23039  0.0  0.0  1.0  0.0\n\n[23040 rows x 4 columns]\nDataFrame X2:\n              0         1         2         3          4          5  \\\n0      1.319439  1.290504  1.303484  1.313838  5126536.0  2800303.0   \n1      1.298357  1.270562  1.298959  1.277435  4389713.0  3869025.0   \n2      1.282172  1.298105  1.287176  1.285875  4225052.0  3701957.0   \n3      1.284140  1.249545  1.284187  1.269528  5294369.0  2927305.0   \n4      1.276229  1.254733  1.278763  1.271525  3748437.0  4127689.0   \n...         ...       ...       ...       ...        ...        ...   \n23035  1.000843  1.003143  1.000842  1.003319    56283.0   105503.0   \n23036  1.000853  1.003033  1.000851  1.003258    56111.0   104254.0   \n23037  1.000861  1.003141  1.000861  1.003494    55964.0   101576.0   \n23038  1.000870  1.003185  1.000869  1.003536    56022.0   101314.0   \n23039  1.000880  1.006761  1.000878  1.010115    52307.0   122044.0   \n\n                6           7          8          9         10         11  \\\n0       3892497.0    562010.0  1145776.0    21842.0  7822897.0  1883040.0   \n1       3189753.0    422286.0  1853373.0    59668.0  6706668.0  1893742.0   \n2       3739205.0    403746.0  1273946.0   112744.0  7022818.0  1541527.0   \n3       5085676.0    371922.0  1888376.0   264956.0  6452352.0  1860876.0   \n4       4256164.0    563946.0  1877691.0   175370.0  7322958.0  2043449.0   \n...           ...         ...        ...        ...        ...        ...   \n23035  56539590.0  22063768.0    88054.0  7598146.0    21134.0  3761393.0   \n23036  56515214.0  22055620.0    86314.0  7600176.0    21732.0  3762098.0   \n23037  56501403.0  22054648.0    85998.0  7597338.0    21883.0  3762879.0   \n23038  56554168.0  22053050.0    86062.0  7597170.0    21550.0  3761025.0   \n23039  48405026.0  18870348.0    90133.0  7620998.0    21482.0  3772082.0   \n\n            12         13       14         15       16           17  \\\n0       9354.0  6679350.0    277.0  7108209.0  17841.0   65843813.0   \n1       9530.0  6739482.0   2322.0  7750939.0  47177.0   64308012.0   \n2      12280.0  5755166.0   6663.0  6821399.0  50473.0   71546240.0   \n3      19414.0  7503502.0   3629.0  6637542.0  98744.0   82921901.0   \n4      48430.0  5772371.0  37353.0  7396741.0  66674.0   70710033.0   \n...        ...        ...      ...        ...      ...          ...   \n23035  10888.0    21699.0  10352.0    39051.0  10352.0  448558876.0   \n23036   9912.0    21935.0  10058.0    39040.0  10575.0  448902079.0   \n23037  10000.0    21477.0  10404.0    39002.0  11976.0  448407155.0   \n23038  10568.0    21633.0  10266.0    39114.0  10878.0  448996860.0   \n23039   9550.0    21555.0  10197.0    39290.0  10542.0  448782413.0   \n\n               18          19  \n0       1703669.0  20418793.0  \n1       4332707.0  19372663.0  \n2       5294928.0  19349228.0  \n3      11407092.0  20379324.0  \n4       8365153.0  19533665.0  \n...           ...         ...  \n23035   1651995.0    871030.0  \n23036   1718312.0    871336.0  \n23037   1981590.0    873024.0  \n23038   1722348.0    871582.0  \n23039   1671382.0    862587.0  \n\n[23040 rows x 20 columns]\nDataFrame X:\n       0_x  1_x  2_x  3_x       0_y       1_y       2_y       3_y          4  \\\n0      1.0  0.0  0.0  0.0  1.319439  1.290504  1.303484  1.313838  5126536.0   \n1      1.0  0.0  0.0  0.0  1.298357  1.270562  1.298959  1.277435  4389713.0   \n2      1.0  0.0  0.0  0.0  1.282172  1.298105  1.287176  1.285875  4225052.0   \n3      1.0  0.0  0.0  0.0  1.284140  1.249545  1.284187  1.269528  5294369.0   \n4      1.0  0.0  0.0  0.0  1.276229  1.254733  1.278763  1.271525  3748437.0   \n...    ...  ...  ...  ...       ...       ...       ...       ...        ...   \n23035  0.0  0.0  1.0  0.0  1.000843  1.003143  1.000842  1.003319    56283.0   \n23036  0.0  0.0  1.0  0.0  1.000853  1.003033  1.000851  1.003258    56111.0   \n23037  0.0  0.0  1.0  0.0  1.000861  1.003141  1.000861  1.003494    55964.0   \n23038  0.0  0.0  1.0  0.0  1.000870  1.003185  1.000869  1.003536    56022.0   \n23039  0.0  0.0  1.0  0.0  1.000880  1.006761  1.000878  1.010115    52307.0   \n\n               5  ...         10         11       12         13       14  \\\n0      2800303.0  ...  7822897.0  1883040.0   9354.0  6679350.0    277.0   \n1      3869025.0  ...  6706668.0  1893742.0   9530.0  6739482.0   2322.0   \n2      3701957.0  ...  7022818.0  1541527.0  12280.0  5755166.0   6663.0   \n3      2927305.0  ...  6452352.0  1860876.0  19414.0  7503502.0   3629.0   \n4      4127689.0  ...  7322958.0  2043449.0  48430.0  5772371.0  37353.0   \n...          ...  ...        ...        ...      ...        ...      ...   \n23035   105503.0  ...    21134.0  3761393.0  10888.0    21699.0  10352.0   \n23036   104254.0  ...    21732.0  3762098.0   9912.0    21935.0  10058.0   \n23037   101576.0  ...    21883.0  3762879.0  10000.0    21477.0  10404.0   \n23038   101314.0  ...    21550.0  3761025.0  10568.0    21633.0  10266.0   \n23039   122044.0  ...    21482.0  3772082.0   9550.0    21555.0  10197.0   \n\n              15       16           17          18          19  \n0      7108209.0  17841.0   65843813.0   1703669.0  20418793.0  \n1      7750939.0  47177.0   64308012.0   4332707.0  19372663.0  \n2      6821399.0  50473.0   71546240.0   5294928.0  19349228.0  \n3      6637542.0  98744.0   82921901.0  11407092.0  20379324.0  \n4      7396741.0  66674.0   70710033.0   8365153.0  19533665.0  \n...          ...      ...          ...         ...         ...  \n23035    39051.0  10352.0  448558876.0   1651995.0    871030.0  \n23036    39040.0  10575.0  448902079.0   1718312.0    871336.0  \n23037    39002.0  11976.0  448407155.0   1981590.0    873024.0  \n23038    39114.0  10878.0  448996860.0   1722348.0    871582.0  \n23039    39290.0  10542.0  448782413.0   1671382.0    862587.0  \n\n[23040 rows x 24 columns]\nX:\n[[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 6.58438130e+07\n  1.70366900e+06 2.04187930e+07]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 6.43080120e+07\n  4.33270700e+06 1.93726630e+07]\n [1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 7.15462400e+07\n  5.29492800e+06 1.93492280e+07]\n ...\n [0.00000000e+00 0.00000000e+00 1.00000000e+00 ... 4.48407155e+08\n  1.98159000e+06 8.73024000e+05]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00 ... 4.48996860e+08\n  1.72234800e+06 8.71582000e+05]\n [0.00000000e+00 0.00000000e+00 1.00000000e+00 ... 4.48782413e+08\n  1.67138200e+06 8.62587000e+05]]\nLabels: 23040\nInputs: 23040\nX_Train: 16128\nX_Test: 6912\nX_Train: 16128\nX_Test: 6912\n"
    }
   ],
   "source": [
    "# Encode class DSCR as integer values\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(X1)\n",
    "encoded_X1 = encoder.transform(X1)\n",
    "\n",
    "# Convert integers to dummy variables (one hot encoded).\n",
    "dummy_x = np_utils.to_categorical(encoded_X1).astype(float)\n",
    "\n",
    "# Convert to dataframe to merge columns\n",
    "dataframe_X1 = pd.DataFrame(dummy_x);\n",
    "dataframe_X2 = pd.DataFrame(X2);\n",
    "\n",
    "print(\"DataFrame X1:\")\n",
    "print(dataframe_X1)\n",
    "print(\"DataFrame X2:\")\n",
    "print(dataframe_X2)\n",
    "\n",
    "dataframe_X = pd.DataFrame.merge(dataframe_X1, dataframe_X2,left_index=True, right_index=True) # Merge model inputs\n",
    "\n",
    "# This will be the number of imputs of the model\n",
    "num_inputs = len(dataframe_X.columns)\n",
    "\n",
    "print(\"DataFrame X:\")\n",
    "print(dataframe_X)\n",
    "\n",
    "# Converting back to <class 'numpy.ndarray'>\n",
    "X = dataframe_X.values;\n",
    "print(\"X:\")\n",
    "print(X)\n",
    "\n",
    "# Labels and inputs must be the same size\n",
    "print(\"Labels: \"+str(len(Y))) # Model label training input\n",
    "print(\"Inputs: \"+str(len(X))) # Model inputs\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Taining and test split-> 30% test - 70% training\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=seed)\n",
    "\n",
    "# Length of train and test data\n",
    "print(\"X_Train: \"+str(len(X_train)))\n",
    "print(\"X_Test: \"+str(len(X_test)))\n",
    "print(\"X_Train: \"+str(len(Y_train)))\n",
    "print(\"X_Test: \"+str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of inputs for the model: 24\n"
    }
   ],
   "source": [
    "print(\"Number of inputs for the model: \"+str(num_inputs))\n",
    "# Model definition as function\n",
    "def PC_IBM_model():\n",
    "\t# Create model\n",
    "\tmodel = Sequential()\n",
    "\t# With the actual encoding input_dim must be more than 20 because there are 4 inputs for dscr (one hot)\n",
    "\tmodel.add(Dense(8, input_dim=num_inputs, activation='relu'))\n",
    "\tmodel.add(Dense(6, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (6,) but got array with shape (15191,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f55ba255e0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPC_IBM_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (6,) but got array with shape (15191,)"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=PC_IBM_model, epochs=200, batch_size=2, verbose=0)\n",
    "estimator.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, X_train, Y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = estimator.predict(X_test)\n",
    "print(predictions)\n",
    "print(encoder.inverse_transform(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}